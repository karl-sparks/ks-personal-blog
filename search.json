[
  {
    "objectID": "posts/ai-powered-development/index.html",
    "href": "posts/ai-powered-development/index.html",
    "title": "AI Powered Development",
    "section": "",
    "text": "AI Resume Builder is gone; long live Sparks AI.\nWhile initially I was thinking to build a dedicated AI to help review and build resumes, in true agile fashion I’ve decided to prioritize getting a functional end-to-end AI prototype up and running first, one that uses the ChatGPT API to generate text. Later, I can shift my focus to incorporating additional features, like resume review and building.\nGuess what? I’ve done it! I’ve created a working web app using Python’s Flask framework. Here’s where Sparks AI stands currently:\nThese three features provide a strong foundation to build upon. My immediate priorities are:"
  },
  {
    "objectID": "posts/ai-powered-development/index.html#case-study-chat-front-end",
    "href": "posts/ai-powered-development/index.html#case-study-chat-front-end",
    "title": "AI Powered Development",
    "section": "Case study: Chat Front-end",
    "text": "Case study: Chat Front-end\nOne of the tasks I had little experience with was front-end development, especially in rendering the chat interface (R Shiny and Python Streamlit are the closest I’ve come before). While I had a general idea of what I wanted, implementing it was a challenge. To kickstart the process, I shamelessly borrowed code from a comprehensive tutorial on implementing ChatterBot in Flask this one was where I grabbed the initial template.\n\n\n\nVersion 1\n\n\nThis version had a few issues:\n\nMinimal styling with a simple title.\nAbsence of chat bubbles.\nUser and AI messages styled the same and positioned on the same side of the chatbox.\n\nTo address these issues, I pasted my entire HTML file into ChatGPT 3.5 and asked it to enhance the styling. Here’s the result.\n\n\n\nVersion 2\n\n\nThis version had improved overall styling but still faced issues 2 and 3 as mentioned earlier. So, I pasted the HTML again and requested GPT-3.5 to add chat bubbles and format them on the left for the bot and right for the human. This approach worked fantastically, as shown below.\n\n\n\nVersion 3\n\n\nSatisfied, I began experimenting with my new chat interface for Sparks AI instead of using ChatGPT 3.5. This experiment quickly brought two more issues to light:\n\nUser-input HTML in the HTML file wasn’t escaped, leaving room for potential injection attacks.\nThe AI chat responses weren’t properly escaped or formatted.\n\nGPT 3.5 easily resolved issue 1 by adding a JavaScript function to escape user HTML input before incorporating it into the chat bubbles. However, it struggled to address issue 2, initially adding a function that took the AI response and converted it to markdown from html, which would render the messages invalid on the actual page since it needed HTML.\nExperimenting with this I realised there were two main issues that each had to be addressed:\n\nthe rendering of the AI response on the HTML page correctly\nthe format of the initial response from the OpenAI API calls\n\nI discovered that each issue had multiple solutions, but the optimal solution heavily relied on my approach to each problem. Initially, GPT-3.5 (and some experiments with GPT-4) had difficulty proposing a solution, but once I directed it on which method to solve each problem, it quickly provided a workable answer for each. In short, GPT-3.5 quickly provided answers to solutions, but only once the problem had been sufficently simplified and defined clearly.\nAs for the solutions, I opted to first use prompt engineering by adding a command to respond in markdown format to the prompt template for any queries to the API. Then, in the Python backend I converted the AI markdown response into HTML, which was sent directly to the front-end client and displayed as is. I found this to be the most reliable way to format the responses effectively.\n\n\n\nVersion 4\n\n\nFinally, I added some CSS styling to enhance the appearance of the code blocks. Chat GPT 3.5 assisted with this, saving me considerable time that I would have spent on online research. Take a look below for the final result.\n\n\n\nVersion 5"
  },
  {
    "objectID": "posts/ai-resume-builder-project-plan/index.html",
    "href": "posts/ai-resume-builder-project-plan/index.html",
    "title": "Hello Resume Builder",
    "section": "",
    "text": "As a first AI project, I thought it fitting that I build an AI to build resumes.\nMy objectives with this project are:\n\nEnhance my skills in developing a comprehensive application from start to finish, including deployment.\nCreate a practical and valuable tool as the project outcome.\n\nTo achieve these objectives, I explored various approaches. Initially, I considered using a straightforward Google Colab notebook, which would effectively address objective 2 by providing the necessary functionality. However, it would not fulfill objective 1.\nTo address both objectives comprehensively, I will develop a more intricate yet robust application using cloud technologies. A simplified diagram is provided below, outlining the primary components:\n\n\n\nSimple Architecture\n\n\nThe main components are:\n\nServerless Backend: This component will be constructed using AWS Lambda, and its primary function will be to facilitate API access to the underlying model.\n\nThe advantages of utilizing serverless backends include scalability for the final app in a resource-efficient manner, as they separate the resource intensive aspects from the continuously running server.\nThe logic of the underlying model will be further refined in proof-of-concept (POC) notebooks. It is likely to break down the resume generation process into multiple components, utilizing ChatGPT to build each part. Additional context will be sourced from the user and/or job advertisements. Another model may be employed to assess the quality of the output.\n\nResume Builder App: This component will primarily focus on the user interface (UI) and will likely be built using Streamlit for efficiency. Its role will be to gather user data and present the generated resumes to users.\nChatGPT: The core model for resume generation will be ChatGPT by OpenAI, which will be accessed through OpenAI’s API.\nVarious datasets stored in AWS: Depending on the progress of the proof-of-concept (POC) work, additional datasets will be collected and stored in AWS. These datasets may encompass:\n\nJob ads\nResumes\nLinkedin profiles etc\n\n\nI anticipate that this approach will serve as a valuable opportunity for honing my skills in the development and deployment of machine learning applications while simultaneously delivering a practical tool. Throughout this endeavor, I intend to maintain a flexible approach, allowing the final design and choice of tools to evolve based on insights gained from experimentation and proof-of-concept work. I will periodically document my progress towards achieving these goals by sharing POC updates on this blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Karl Sparks",
    "section": "",
    "text": "I am a AI enthusiast and data expert. I am currently a Principal Analyst at the Australian Prudential Regulation Authority where I generate insights and create data tools.\nPreviously, I worked at the Reserve Bank of Australia where I maintained the RBA’s banks’ funding model and undertook economic and financial research."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Karl Sparks",
    "section": "Projects",
    "text": "Projects\n\nSparks AI"
  },
  {
    "objectID": "index.html#articles",
    "href": "index.html#articles",
    "title": "Karl Sparks",
    "section": "Articles",
    "text": "Articles\n\nBank Fees in Australia\nBank Fees in Australia During the COVID-19 Pandemic"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Building a Basic AI Chat Bot Web App using FastAPI, HTMX, and Docker\n\n\n\n\n\n\n\nsparks-ai\n\n\ntutorial\n\n\nfastapi\n\n\nhtmx\n\n\ndocker\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nKarl Sparks\n\n\n\n\n\n\n  \n\n\n\n\nAI Powered Development\n\n\n\n\n\n\n\nsparks-ai\n\n\npoc\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nKarl Sparks\n\n\n\n\n\n\n  \n\n\n\n\nProof of Concept\n\n\n\n\n\n\n\nresume-builder\n\n\npoc\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nKarl Sparks\n\n\n\n\n\n\n  \n\n\n\n\nHello Resume Builder\n\n\n\n\n\n\n\nresume-builder\n\n\ndesign\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nKarl Sparks\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fastapi-htmx-docker-tutorial/index.html",
    "href": "posts/fastapi-htmx-docker-tutorial/index.html",
    "title": "Building a Basic AI Chat Bot Web App using FastAPI, HTMX, and Docker",
    "section": "",
    "text": "This tutorial will guide you through building a basic AI chat bot using the following technology stack:\n\nFastAPI;\n\nFor the backend API of the chatbot, and in conjunction with Jinja2, to render the website.\n\nHTMX;\n\nHTMX was chosen because it provides a simplified approach to create dynamic websites without the need for a frontend framework.\n\ndocker; and\n\nDocker will be used to containerize the application.\n\nGPT-4\n\nGPT-4, accessed via the langchain package, will serve as our underlying model."
  },
  {
    "objectID": "posts/fastapi-htmx-docker-tutorial/index.html#installation",
    "href": "posts/fastapi-htmx-docker-tutorial/index.html#installation",
    "title": "Building a Basic AI Chat Bot Web App using FastAPI, HTMX, and Docker",
    "section": "Installation",
    "text": "Installation\nExecute the command below to install FastAPI, Uvicorn, and Jinja2. Uvicorn will serve the FastAPI application, while Jinja2, a templating engine, will help render the website.\npip install fastapi uvicorn['standard'] jinja2"
  },
  {
    "objectID": "posts/fastapi-htmx-docker-tutorial/index.html#create-the-fastapi-application",
    "href": "posts/fastapi-htmx-docker-tutorial/index.html#create-the-fastapi-application",
    "title": "Building a Basic AI Chat Bot Web App using FastAPI, HTMX, and Docker",
    "section": "Create the Fastapi application",
    "text": "Create the Fastapi application\nMake a new file named main.py and add the provided code to it.\nfrom fastapi import FastAPI, Request\n\napp = FastAPI() # Create the Fastapi application\n\n@app.get(\"/\") # Create a route for the root of the website\ndef index(request: Request): # Define the function that will be called when the route is hit\n    return {\"Hello\": \"World\"} # Return a simple hello world json response\nThis initializes the basic FastAPI application. Setting up a basic web app is straightforward, requiring only a few lines of code.\nTo start the web app, execute the following:\nuvicorn main:app --reload\nHere, main refers to the main.py file, and app points to the FastAPI application initialized within main.py. The --reload flag automatically reloads the server upon detecting code modifications, making development smoother. If you run the above command and then navigate to http://127.0.0.1:8000 you should be greeted by the Hello World json response. One nefty feature of fastapi is the automatic docs generated. You can find them at http://127.0.0.1:8000/docs, which will not only allow you to see the documenation but also test the API and its responses."
  },
  {
    "objectID": "posts/first-poc/index.html",
    "href": "posts/first-poc/index.html",
    "title": "Proof of Concept",
    "section": "",
    "text": "Key Findings from Proof of Concept:\n\nLLMs for Resume Building: LLMs, specifically GPT-4, proved effective in summarizing information for resume creation.\nContext Sensitivity: Output quality was sensitive to both context provided and the prompt template used.\nImprovement Aspects: To enhance output quality, three key aspects were identified:\n\n\nValidation Mechanism: Essential for testing model changes and identifying better-performing versions, potentially using another LLM for validation.\nFeedback Mechanism: Vital for refining the model and validation process based on user-detected errors and issues.\nStructured Output: Implementing structured output to ensure consistency in the generated resumes.\n\n\nProof of concept\nFirst, I used ChatGPT-4 to make sure the use case is workable.\nAlthough, GPT-4 is one of the most expensive LLMs available, I decided to go with GPT-4 because it is generally the best performing model for most use cases. If I can’t get my use case working well with GPT-4 than I would unlikely be able to get it working with another model. The cost is also not that large at the proof of concept stage, although would be significant if scaled up.\nFor this proof of concept I will use Langchain to access GPT-4 and create the prompts. This API conveniently lets me set up the prompt in components based on a template. This lets met feed in additional information piece by piece, such as job description and resume experience. This functionality will be important when it comes to generallising this model to other jobs/resumes etc.\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport openai\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nllm = ChatOpenAI(model_name=\"gpt-4\")\n\nI create the prompt template below. When creating the template I experimented with a few different options to improve the outcome. This included using common recommendations such as including “lets think step by step”, and specifying the output format.\n\ntemplate = \"\"\"You are helping create a resume for a job seeker (the candidate).\n\nUse the following job description and information on the candidate background when answering the below question.\n\nMake sure you do not make specific reference to skills or achievements that are not mentioned in the background.\n\nThe candidate is applying for a job as a {job_title} at a {company_name} with the job description as follows:\n{job_description}\n\nCandidate background: \n{context}\n\nQuestion: Create a resume for the candedate. Make sure to include measureable metrics where possible.\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"context\", \"job_title\", \"company_name\", \"job_description\"])\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\njob_title = \"Machine Learning Engineer\"\n\ncompany_name = \"Machinify\"\n\njob_description = \"\"\"\nMachinify is a revolutionary healthcare software company with a mission to ensure that patients get the right medical treatment, at the right time, at the right price. Our cloud-based Machinify AI platform leverages the latest advances in machine learning, large language models, data analytics, and cloud processing to solve previously intractable problems, transforming healthcare administration and payment operations.\n\nWe are seeking an ML Engineer to join our AI focused healthcare startup. Our team has deployed models that check for anomalies in medical claims, predict the trajectory of care of a particular patient and the next likely treatment and even “read and understand” medical record documents using state-of-the-art NLP and computer vision techniques to assist clinicians. There are many more such problems to be solved. So if you are looking for a place where you can apply your expertise in AI/ML to solve real problems and make a difference in an industry that impacts millions of lives while accelerating your career, you’ll love Machinify. We are seeking a Sr Machine Learning Scientist to join our team.\n\nWhat you’ll do:\nAdvance Machinify’s capabilities to model and understand medical decisions which will power a range of products in the claims processing space. \nMake thoughtful decisions around which methods/algorithms are likely to work well in solving the business problem at hand\nConsider the quality of the available input data and build robust systems that will work well in the presence of noise/errors\nMeasure the model output in practical real-world settings and iterate your approach \nLearn about the healthcare industry and become an expert over time\n\nWhat You Bring:\nYou enjoy solving real-world business problems by developing, from scratch, ML models and deploying them in production - and have been doing that successfully for a while. \nYou are comfortable measuring and optimizing the direct business impact of your work. \nYou are interested in learning about the healthcare industry and helping us improve the care millions of people receive. \nYou are scrappy, and love solving hard problems that matter. \nYou are experienced with SQL, handling large-scale data, and are comfortable with at least one programming language (Python, R, etc.). \nYou have experience building ML models using modern ML approaches like Neural Nets or Tree-ensembles from scratch for new applications - making decisions relating to which supervised labels to use, the metric to optimize for, and the features likely to be useful. \nYou are a critical thinker who can be strategic without losing attention to detail. \nYou are comfortable taking the initiative and owning projects from start to finish. \nYou can build positive relationships based on trust and value delivered. \n\nThe base salary for this position will vary based on an array of factors unique to each candidate such as qualifications, years and depth of experience, skill set, certifications, etc. The base salary range for this role is $180k-250k. We are hiring for different seniorities, and our Recruiting team will let you know if you qualify for a different role/range. Salary is one component of the total compensation package, which includes meaningful equity, excellent healthcare, flexible time off, and other benefits and perks.\n\nEqual Employment Opportunity at Machinify\n\nMachinify is committed to hiring talented and qualified individuals with diverse backgrounds for all of its positions. Machinify believes that the gathering and celebration of unique backgrounds, qualities, and cultures enriches the workplace.\n\"\"\"\n\ncontext = \"\"\"\nWork Experience:\n  - Principal Analyst for the Austrlian Prudential Regulation Authority.\n    - Extensively uses R and SQL to analyse data and produce reports.\n    - Has a strong understanding of the Australian financial system.\n    - Created R packages to automate the production of reports.\n    - Created new automated reports to highlight the difference between old and new capital frameworks for everyone of the 114 reporting ADIs.\n  - Senior Analyst for the Reserve Bank of Australia.\n    - Used R to build and maintain models to monitor and forecast banks' funding costs.\n    - Undertook economic and financial research to support the Bank's monetary policy and payment regulation functions.\n    - Liaising with external and internal stakeholders for their expert opinions on different bank funding markets, drawing this all together to provide a comprehensive overview of funding conditions.\n    - Monitoring of the quantity and price of bank funding sources such as deposits, bonds, securitisation and money markets using various data sources.\n    - Analysing data collected through the Economic and Financial Statistics (EFS) collection. Including liaising with reporting institutions on reporting standards and data quality issues, and preparing data for publication.\n    - Used R and SQL to build and maintain models to monitor and forecast banks' funding costs. to analyse data and forecasting the impact of changes in banks' funding conditions on average funding costs.\n\"\"\"\n\nThe above prompt and context will create the below response from ChatGPT-4.\n\nanswer = llm_chain.run({\"context\": context, \"job_title\": job_title, \"company_name\": company_name, \"job_description\": job_description})\n\nprint(answer)\n\nResume\n\nName: [Candidate's name]\nContact Information: [Candidate's Email, Phone, LinkedIn]\n\nObjective: Motivated and experienced analyst, leveraging expertise in data analysis, machine learning, and healthcare industry to drive improvements in patient care and administrative efficiency. \n\nWork Experience:\n\nPrincipal Analyst, Australian Prudential Regulation Authority\n- Utilized R and SQL extensively for data analysis and report generation, enhancing the understanding of the Australian financial system.\n- Automated report production through the creation of R packages, increasing efficiency and accuracy.\n- Developed novel automated reports to differentiate between old and new capital frameworks for all 114 reporting ADIs, providing crucial insights for policy making.\n\nSenior Analyst, Reserve Bank of Australia\n- Built and maintained models using R and SQL to monitor and forecast banks' funding costs, contributing to the accuracy of financial planning.\n- Conducted economic and financial research to support the Bank's monetary policy and payment regulation functions.\n- Liaised with a variety of stakeholders for expert opinions on different bank funding markets, culminating in a comprehensive overview of funding conditions.\n- Monitored the quantity and price of bank funding sources using various data sources to forecast changes in banks' funding conditions on average funding costs.\n- Handled data collected through the Economic and Financial Statistics (EFS) collection, liaising with reporting institutions on reporting standards and data quality issues, and preparing data for publication.\n\nSkills:\n- Proficient in R and SQL.\n- Extensive experience in data analysis and financial modeling.\n- Excellent communication skills, with a track record of effectively liaising with both internal and external stakeholders.\n- Proven ability to automate report production and increase efficiency.\n- Strong problem-solving and critical-thinking abilities.\n\nEducation:\n[Relevant degree(s) and institution(s)]\n\nCertifications:\n[Relevant certifications]\n\nLanguages:\n- English\n- [Any other languages the candidate speaks]\n\nReferences:\nAvailable upon request"
  }
]